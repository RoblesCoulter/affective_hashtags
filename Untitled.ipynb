{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CULTURAL #NOWPLAYING DATA\n",
    "*********************************\n",
    "\n",
    "This dataset contains data describing the listening events of users (extracted from the #nowplaying dataset), the emotion extracted from the hashtags used in the according tweets and information about the location of the user.\n",
    "\n",
    "\n",
    "TWITTER AND TRACK DATA\n",
    "-------------------------------\n",
    "The data regarding the listening events is contained in np_cultural.zip and is encoded as json. It holds the following information:\n",
    "\n",
    "- id: the id of the underlying tweet [*]\n",
    "- user_id: the id of the user who sent the tweet (MD5 of it)\n",
    "- user_lang: The BCP 47 code for the userâ€™s self-declared user interface language. [*]\n",
    "- user_time_zone: [*]\n",
    "- text: actual content of the tweet [*]\n",
    "- tweet_lang: language of the tweet (as detected by Twitter; BCP 47 language identifier corresponding to the machine-detected language of the Tweet text, or und if no language could be detected.) [*]\n",
    "- geo: Deprecated version of coordinates (however, we deal with data stemming from before this API change, therefore we still include it; cf. coordinates for a description)\n",
    "- coordinates: Represents the geographic location of this Tweet as reported by the user or client application. The inner coordinates array is formatted as geoJSON (longitude first, then latitude). [*]\n",
    "- place: When present, indicates that the tweet is associated (but not necessarily originating from) a Place. [*]\n",
    "- created_at: time the tweet was sent. [*]\n",
    "- source: Utility used to post the Tweet, as an HTML-formatted string. [*]\n",
    "- track_title: title of the track the user tweeted about\n",
    "- track_id: the unique id of the track (from #nowplaying dataset) \n",
    "- artist_name: name of artist performing the track\n",
    "- artist_id: the unique id of the artist (from #nowplaying dataset) \n",
    "- hashtags: list of hashtags used in the tweet.\n",
    "\n",
    "[*] for further information about the information gathered from Twitter, please consult https://dev.twitter.com/overview/api/tweets\n",
    "\n",
    "Please note that we do only add key-value pairs for geo/coordinates/place information if this information was provided by the Twitter API (i.e., missing keys signal that this information is not available for the given tweet).\n",
    "\n",
    "\n",
    "SENTIMENT DATA\n",
    "-------------------------------\n",
    "The data regarding the hashtag's sentiment (if any could be obtained) is contained in np_cultural_sentiment.csv and is formatted as csv. The sentiment score was obtained by applying a set of well-known sentiment dictionaries. The sentiment scores are scaled between 0 and 1 (very negative to very positive). For each dictionary, we list the minimum, maximum, sum and average sentiment score across all hashtags used within every tweet (most tweets only feature a single hashtag we can assign a sentiment value to)\n",
    "It contains the following information (in this particular order):\n",
    "- name of the hashtag\n",
    "- AFINN dictionary (min, max, sum, avg)\n",
    "- Opinion Lexicon (min, max, sum, avg)\n",
    "- Sentistrength Lexicon (min, max, sum, avg)\n",
    "- Vader (min, max, sum, avg)\n",
    "- Sentiment Hashtag Lexicon (min, max, sum, avg)\n",
    "\n",
    "\n",
    "Please note that we only added hashtags for which we could obtain a sentiment value from at least one sentiment dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "import csv\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Call Mongo Instance\n",
    "client = MongoClient()\n",
    "db = client.now_playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x114b84948>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NowPlaying Cultural Sentiment Data\n",
    "cs_header = [\"hashtag\", \"AFINN_min\", \"AFINN_max\", \"AFINN_sum\", \"AFINN_avg\", \"OpinionLex_min\", \"OpinionLex_max\", \"OpinionLex_sum\", \"OpinionLex_avg\", \"Sentistrength_min\", \"Sentistrength_max\", \"Sentistrength_sum\", \"Sentistrength_avg\", \"Vader_min\", \"Vader_max\", \"Vader_sum\", \"Vader_avg\", \"SentimentHashtag_min\", \"SentimentHashtag_max\", \"SentimentHashtag_sum\", \"SentimentHashtag_avg\"]\n",
    "np_cs = pd.read_csv('np_cultural_sentiment.csv',names = cs_header)\n",
    "\n",
    "#IMPORTING Cultural Sentiment Data to MongoDB\n",
    "np_cs_json = json.loads(np_cs.to_json(orient=\"records\"))\n",
    "db.cultural_sentiment.drop()\n",
    "db.cultural_sentiment.insert_many(np_cs_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x20a6a9438>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transformation of NowPlaying Cultural Tweets to DataframeStructure\n",
    "output = []\n",
    "with open(\"np_cultural.json\") as f:\n",
    "    for line in f:     \n",
    "        output.append(json.loads(line))\n",
    "np_c = pd.DataFrame(output)\n",
    "#IMPORTING Cultural Tweets to MongoDB\n",
    "db.cultural_tweet.drop()\n",
    "records = json.loads(np_c.T.to_json()).values()\n",
    "db.cultural_tweet.insert_many(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#db.cultural_tweet.find({\"hashtag\": \"nobeats\"})\n",
    "user_tweets = db.cultural_tweet.find({\"user_id\": '1c10f9788fdcc4baf6cf6a2631fe78bc12102418'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564301"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOTAL Number of Tweets\n",
    "db.cultural_tweet.find({}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9431"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids = db.cultural_tweet.distinct(\"user_id\")\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for user_id in user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
